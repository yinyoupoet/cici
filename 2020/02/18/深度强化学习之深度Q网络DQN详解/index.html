<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>深度强化学习之深度Q网络DQN详解 | yinyoupoet的博客</title><meta name="description" content="DQN详细介绍"><meta name="keywords" content="AI,深度学习,强化学习"><meta name="author" content="yinyoupoet"><meta name="copyright" content="yinyoupoet"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="https://yinyoupoet.github.io/2020/02/18/深度强化学习之深度Q网络DQN详解/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="深度强化学习之深度Q网络DQN详解"><meta name="twitter:description" content="DQN详细介绍"><meta name="twitter:image" content="https://yinyoupoet.github.io/images/cover&amp;top/DQN.png"><meta property="og:type" content="article"><meta property="og:title" content="深度强化学习之深度Q网络DQN详解"><meta property="og:url" content="https://yinyoupoet.github.io/2020/02/18/深度强化学习之深度Q网络DQN详解/"><meta property="og:site_name" content="yinyoupoet的博客"><meta property="og:description" content="DQN详细介绍"><meta property="og:image" content="https://yinyoupoet.github.io/images/cover&amp;top/DQN.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="机器学习笔记" href="https://yinyoupoet.github.io/2020/02/19/机器学习笔记/"><link rel="next" title="WireShark的使用" href="https://yinyoupoet.github.io/2020/02/18/WireShark的使用/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?19446aca39e2fe294709670cbf706533";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta name="google-site-verification" content><meta name="baidu-site-verification" content><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"207CQUEWZA","apiKey":"e43d6394203388cf47dc525ade9c9210","indexName":"HexoBlog","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://yinyoupoet.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天'

  
}</script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#强化学习"><span class="toc-number">1.</span> <span class="toc-text"> 强化学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#q-learning"><span class="toc-number">2.</span> <span class="toc-text"> Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#引例"><span class="toc-number">2.1.</span> <span class="toc-text"> 引例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#q值更新方法"><span class="toc-number">2.2.</span> <span class="toc-text"> Q值更新方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#s-a表格如何更新"><span class="toc-number">2.2.1.</span> <span class="toc-text"> S-A表格如何更新</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#策略policy选择注意事项"><span class="toc-number">2.3.</span> <span class="toc-text"> 策略Policy选择注意事项</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dqn"><span class="toc-number">3.</span> <span class="toc-text"> DQN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#dnn如何训练"><span class="toc-number">3.1.</span> <span class="toc-text"> DNN如何训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experience-reply"><span class="toc-number">3.2.</span> <span class="toc-text"> Experience Reply</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#target-network"><span class="toc-number">3.3.</span> <span class="toc-text"> Target Network</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考资料"><span class="toc-number">4.</span> <span class="toc-text"> 参考资料</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/images/cover&amp;top/DQN.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">yinyoupoet的博客</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><div class="menu_mask"></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" src="/images/cover&top/tenor.gif" data-original="/img/avatar1.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item text-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">15</div></a></div></div><div class="mobile_data_item text-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item text-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">9</div></a></div></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-picture-o"></i><span> 相册</span></a><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a><a class="site-page" href="/categories/诗歌/"><i class="fa-fw fa fa-star-half-o"></i><span> 诗歌</span></a><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title"><div class="posttitle">深度强化学习之深度Q网络DQN详解</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-02-18<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-02-20</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/深度学习/">深度学习</a></span><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3.5k</span><span class="post-meta__separator">|</span><span>阅读时长: 11 分钟</span><span class="post-meta__separator">|</span><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="110" src="//music.163.com/outchain/player?type=0&id=4872331166&auto=1&height=90"></iframe></center>
# 引言
<p>本文将对深度强化学习中经典算法DQN进行详细介绍，先分别介绍强化学习和Q-学习，然后再引入深度强化学习和DQN。本文所有参考资料及部分插图来源均列在文末，在文中不做额外说明。</p>
<h1 id="强化学习"><a class="markdownIt-Anchor" href="#强化学习"></a> 强化学习</h1>
<p>讲强化学习先讲其适用的场景。强化学习多用在需要与环境交互的场景下，即给定一个环境的状态（<code>State</code>），程序根据某种策略（<code>Policy</code>）选出一个对应的行为（<code>Action</code>），而执行这个<code>Action</code>后环境又会发生改变，即状态会转换为新的状态<code>S'</code>，且每执行完一个<code>Action</code>后程序会得到一个激励值（<code>Reward</code>），而程序就依据得到的激励值的大小调整其策略，使得在所有步骤执行完后，即状态到达终止状态（<code>Terminal</code>）时，所获得的<code>Reward</code>之和最大。</p>
<p>上面说的可能比较抽象，举个例子，假如我们的程序是一只小狗，现在我们让它坐下（给它一个<code>State</code>），它如果听话（某种Policy）坐下（执行<code>Action</code>），那么我们就给它一个鸡腿（正激励），而如果它不听话（某种<code>Policy</code>）跑开了（执行另一种<code>Action</code>），我们就罚它一顿不许吃饭（负激励），而在它执行完这个行为后，我们可以再次对它提出要求，比如让它站起来（新的<code>State</code>），然后如此往复。小狗对我们给的每一个状态都要给出一个行为，而我们会在它每次给出行为后决定给它一个什么样的激励，且环境的状态在它执行完<code>Action</code>后可能会发生变化，然后它需要对新环境再继续根据某种策略选择执行新的动作，从而得到新的激励。而我们训练的目的，就是使得<strong>总的激励值之和</strong>最大。</p>
<p>总结一下，在强化学习中，我们关注的有如下几点：</p>
<ul>
<li>环境观测值/状态 <strong>S</strong>tate</li>
<li>动作选择策略 <strong>P</strong>olicy</li>
<li>执行的动作/行为 <strong>A</strong>ction</li>
<li>得到的奖励 <strong>R</strong>eward</li>
<li>下一个状态 <strong>S’</strong></li>
</ul>
<p><img src="/images/cover&top/tenor.gif" data-original="/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_mPGk9WTNNvp3i4-9JFgD3w.png" alt="强化学习流程图"></p>
<p>强化学习执行流程如上图所示，<code>Agent</code>是我们的程序，它观察<code>Environment</code>并获得<code>state</code>，依据它的<code>Policy</code>对<code>state</code>做出<code>action</code>，此时能得到一个<code>reward</code>，且<code>Environment</code>改变了，因此<code>Agent</code>会得到一个新的<code>state</code>，并继续执行下去。</p>
<h1 id="q-learning"><a class="markdownIt-Anchor" href="#q-learning"></a> Q-Learning</h1>
<p>Q学习算法是强化学习中的一种，更准确的说，是一种关于策略的选择方式。实际上，我们可以发现，强化学习的核心和训练目标就是选择一个合适的策略<code>Policy</code>，使得在每个<code>epoch</code>结束时得到的<code>reward</code>之和最大。</p>
<p>Q学习的思想是：<code>Q(S, A)</code> = 在状态<code>S</code>下，采取动作<code>A</code>后，<strong>未来</strong>将得到的奖励<code>Reward</code>值之和。</p>
<h2 id="引例"><a class="markdownIt-Anchor" href="#引例"></a> 引例</h2>
<p>而如果能知道选取那个动作能使得未来得到的奖励之和最大，那么选取哪个动作就很理所当然了。但是，<code>Q</code>函数的值到底是怎么得到的呢？请耐心，答案在下面揭晓。下面先看下面例子，这是一个Flappy Bird小游戏（原网址：<a href="https://enhuiz.github.io/flappybird-ql/" target="_blank" rel="noopener">https://enhuiz.github.io/flappybird-ql/</a>），你可以自己点击屏幕玩这个游戏，也可以点击下方“Enable Q-learning”按钮，用Q-learning算法来自动玩这个游戏，给程序一两分钟，他就能轻易取得超过超过人类的成绩。</p>
<iframe height="680px" width="100%" style="border: 0;" src="https://enhuiz.github.io/flappybird-ql/"></iframe>
这个程序出自知乎上[牛阿](https://www.zhihu.com/people/Enhuiz)大佬之手，其设计思路如下：
<ul>
<li><strong>状态State</strong>：将每一帧作为一个状态，取小鸟离下一个地面上柱子在水平和竖直方向上的距离作为状态的观测值，即下图中的(△x, △y)；</li>
</ul>
<p><img src="/images/cover&top/tenor.gif" data-original="/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/v2-6ccc74c071fd10520ad4190080447bee_hd.jpg" alt="图源见水印"></p>
<ul>
<li>
<p><strong>行为Action</strong>：对每一个状态（每一帧），只有两种选择：跳，不跳；</p>
</li>
<li>
<p><strong>奖励Reward</strong>：小鸟活着时给每帧奖励1，死亡时奖励-1000。</p>
</li>
</ul>
<p>在该游戏中，程序是如何选择该跳还是不该跳呢？按照前面说的Q学习算法，那么它应该是需要有一个<code>Q(S, A)</code>函数的，可以知道在什么状态时采取什么样的行为能得到最大的Reward之和。在这个游戏中，很显然状态和动作的组合都是有限的，因此可以维护一个<code>S-A</code>表，其记录了在每个状态下，采用什么动作时能得到什么样的<code>Q</code>值。表格形式如下，只要程序在运行中不断更新这个表格，使其最终能收敛，难么程序就能拿得到的<code>state</code>通过查表的方式来判断它该选择什么样的行为，才能获得最大的Q值。</p>
<p><img src="/images/cover&top/tenor.gif" data-original="/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/v2-7378f0165d6e13b78778dbed00de6ada_hd.jpg" alt="图源见水印"></p>
<h2 id="q值更新方法"><a class="markdownIt-Anchor" href="#q值更新方法"></a> Q值更新方法</h2>
<p>Q值大体上有两种更新方式，一种是类似上面例子中的情况，状态和行为的组合是可以穷尽的情况，这时候往往采用的是<code>S-A</code>表格的形式记录Q值，而如果状态和行为的组合不可穷尽，比如自动驾驶中输入的外界环境照片与车速之间的组合是有无穷种的，那么前一种方法显然就不适用了，这时候常用的方式为将深度学习与Q学习结合起来，也就是本文的重点，DQN，这个我们将在后面重点讲解。</p>
<h3 id="s-a表格如何更新"><a class="markdownIt-Anchor" href="#s-a表格如何更新"></a> S-A表格如何更新</h3>
<p>对于使用<code>S-A</code>表格的情况，需要如何更新其表格中的Q值，使得其在每一个状态下都能选择总体最优的策略呢？</p>
<p>这里首先引出Q值的更新方法：</p>
<p><img src="/images/cover&top/tenor.gif" data-original="/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/qfunction-650x56.png" alt="Q值更新公式"></p>
<p>解释一下，<code>s</code>为状态State，<code>a</code>为采取的行为Action，<code>α</code>参数用来表示新的值对更新后值所造成的影响大小，<code>r</code>为在状态<code>s</code>下采取动作<code>a</code>后获得的奖励Reward，<code>γ</code>也是一个discount值，即用来减小新值的影响的值。其中<code>α</code>和<code>γ</code>的范围都在<code>0~1</code>之间。</p>
<p>如果还不明白，那么可以将这个公式拆开理解。</p>
<p>先看左边一部分，如果不看<code>(1 - α)</code>，那么就是在状态<code>s</code>下采取动作<code>a</code>时的旧的Q值，乘以<code>(1 - α)</code>是因为要更新它，但是也不能把旧的Q值全盘否定了呀。</p>
<p>再看右边部分，同样先不看<code>α</code>，里面是激励值<code>r</code>加上一部分，而后面那部分先去掉<code>γ</code>也不看，就是在下一个状态下的最大Q值，想一想，状态s下采用动作a后得到的激励值r加上下一个状态下的最大Q值，不就是一个新的Q值么，它这是在不断使得激励值<code>r</code>收敛啊，而像<code>α</code>和<code>γ</code>只是用来控制新的Q值和旧的Q值各占多少权重罢了。</p>
<h2 id="策略policy选择注意事项"><a class="markdownIt-Anchor" href="#策略policy选择注意事项"></a> 策略Policy选择注意事项</h2>
<p>前面讲了，在强化学习中最重要的部分就是策略的选择，<code>S-A</code>表格说白了也不过是给选择哪个策略提供了一个参考。而在实际实验中，如果对每个状态<code>s</code>值，都选择其能获得最大Q值的行为去执行，是有问题的！</p>
<p>假使初始的S-A表格所有值全为0，那么在状态s采用随机一个行为（比如a1），并第一次获得reward后，如果reward值大于0，那么以后再遇见状态s时，程序都会直接采用行为a1，然而，还有很多种没有行为从来都没有常识过，说不定采取其他的行为会使得它能得到的Q值更大。</p>
<p>因此在强化学习中，往往需要设置一个阈值<code>ε</code>来保持一定的随机程度，即在每次做决定前，先生成一个随机数，如果这个随机数比<code>ε</code>小，那么就随机选取一个action，否则才选取当前已知条件下能使得Q值最大的action。这个阈值<code>ε</code>往往一开始被设置地很大，而其值也会随着程序不断地迭代而慢慢衰减，一般也需要给其设置一个最小值，即衰减到最小值后就停止衰减了。这样的好处是使得程序可以遍历所有的<code>S-A</code>对，以准确判断在给定状态下选择哪个行为最优，而这种做法被称为<strong>exploration</strong>，这种算法叫做<strong>e-greddy</strong>。</p>
<h1 id="dqn"><a class="markdownIt-Anchor" href="#dqn"></a> DQN</h1>
<p>终于讲到DQN了，DQN即Deep Q Network，深度Q网络。</p>
<p>DQN属于DRL（深度强化学习）的一种，它是深度学习与Q学习的结合体。前面讲了采用<code>S-A</code>表格的局限性，当状态和行为的组合不可穷尽时，就无法通过查表的方式选取最优的Action了。这时候就该想到深度学习了，想通过深度学习找到最优解在很多情况下确实不太靠谱，但是找到一个无限逼近最优解的次优解，倒是没有问题的。</p>
<p>因此DQN实际上，总体思路还是用的Q学习的思路，不过对于给定状态选取哪个动作所能得到的Q值，却是由一个深度神经网络来计算的了，其流程图如下：</p>
<p><img src="/images/cover&top/tenor.gif" data-original="/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/reinforcement_learning_loop-650x294.png" alt="deep q-learning"></p>
<h2 id="dnn如何训练"><a class="markdownIt-Anchor" href="#dnn如何训练"></a> DNN如何训练</h2>
<p>现在我们的选择哪个动作，是由DNN来做决定的，因此我们需要训练DNN以使其能达到令人满意的表现。这显然是一个监督学习的问题，那么训练集是什么，标签是什么，损失函数又是什么？</p>
<p>首先，我们DNN的输出值，自然是在给定状态的情况下，执行各action后能得到的Q值。然而事实上我们在很多情况下并不知道最优的Q值是什么，比如自动驾驶、围棋等情况，所以似乎我们没法给出标签。但是什么是不变的呢？<strong>Reward</strong>！</p>
<p>对状态s，执行动作a，那么得到的reward是一定的，而且是不变的！因此需要考虑从reward下手，让预测Q值和真实Q值的比较问题转换成让模型实质上在拟合reward的问题。</p>
<p>如果不能很好的理解，请看下面公式，下面公式中target是什么我们会在后面说到，这里先忽略它，那个红色的<code>θi-</code>我们也暂且将它当成<code>θi</code>来看。这个公式描述的就是模型的损失函数，大括号外面就是求一个均方差，我们主要看括号里面。前面被target标出来的地方是这一步得到的reward+下一状态所能得到的最大Q值，它们减去这一步的Q值，那么实际上它们的差就是实际reward减去现有模型认为在s下采取a时能得到的reward值。</p>
<p><img src="/images/cover&top/tenor.gif" data-original="/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_YCgMUijhU4p_y3sctvu-kQ.png" alt="损失函数"></p>
<p>现在的问题就已经转换为需要一组训练集，它能够提供一批四元组（s, a, r, s’），其中s’为s执行a后的下一个状态。如果能有这样一个四元组，就能够用来训练DNN了，这就是我们要介绍的<strong>experience reply</strong>。</p>
<h2 id="experience-reply"><a class="markdownIt-Anchor" href="#experience-reply"></a> Experience Reply</h2>
<p>前面提到我们需要一批四元组（s, a, r, s’）来进行训练，因此我们需要缓存一批这样的四元组到经验池中以供训练之用。由于每次执行一个动作后都能转移到下一个状态，并获得一个reward，因此我们每执行一次动作后都可以获得一个这样的四元组，也可以将这个四元组直接放入经验池中。</p>
<p>我们知道这种四元组之间是存在关联性的，因为状态的转移是连续的，如果直接按顺序取一批四元组作为训练集，那么是容易过拟合的，因为训练样本间不是独立的！为解决这个问题，我们可以简单地从经验池中随机抽取少量四元组作为一个batch，这样既保证了训练样本是<strong>独立同分布</strong>的，也使得每个batch<strong>样本量不大</strong>，能加快训练速度。</p>
<p>训练的伪代码如下图（图源知乎）：</p>
<p><img src="/images/cover&top/tenor.gif" data-original="/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/v2-28258bf55770f6b5d1dc4077d5bad64e_hd.jpg" alt="DQN伪代码"></p>
<h2 id="target-network"><a class="markdownIt-Anchor" href="#target-network"></a> Target Network</h2>
<p>上面的代码似乎已经能够正常运行了，为什么又冒出一个target network呢？回想下前面那个公式，这里重新搬到下面来，是不是之前说target和<code>θi-</code>都先忽略，现在就该解释一下了。</p>
<p><img src="/images/cover&top/tenor.gif" data-original="/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/1_YCgMUijhU4p_y3sctvu-kQ.png" alt="损失函数"></p>
<p>这个公式里<code>θi-</code>和<code>θi</code>肯定是有区别的，不然也犯不着用两个符号了。事实上，我们需要设计两个DNN，它们结构完全一样，但是参数不一样，即神经网络中各层的权重、偏置等，一个的参数是<code>θi-</code>，而另一个是<code>θi</code>。且规定每运行C步后让<code>θi- = θi</code>。</p>
<p>为什么要弄这么奇怪的东西？</p>
<p><img src="/images/cover&top/tenor.gif" data-original="/images/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9CDQN%E8%AF%A6%E8%A7%A3/v2-4249c553d14aca24ebce4db92d8f34c6_hd.jpg" alt="DQN伪代码"></p>
<h1 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h1>
<ul>
<li>Ye, Hao, Geoffrey Ye Li, and Biing-Hwang Fred Juang. “Deep reinforcement learning based resource allocation for V2V communications.” <em>IEEE Transactions on Vehicular Technology</em> 68.4 (2019): 3163-3173.</li>
<li>Mnih, Volodymyr, et al. “Human-level control through deep reinforcement learning.” <em>Nature</em> 518.7540 (2015): 529-533.</li>
<li><a href="https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b" target="_blank" rel="noopener">Welcome to Deep Reinforcement Learning Part 1 : DQN</a></li>
<li><a href="https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4" target="_blank" rel="noopener">RL — DQN Deep Q-network</a></li>
<li><a href="https://www.zhihu.com/question/278182581/answer/992280312" target="_blank" rel="noopener">关于强化学习中经验回放（experience replay）的两个问题？ - 宝珠道人的回答 - 知乎</a></li>
<li><a href="https://www.zhihu.com/question/266391390/answer/307198280" target="_blank" rel="noopener">memory  replay 是不是就是在DQN中为训练提供训练样本的呢？ - 东林钟声的回答 - 知乎</a></li>
<li><a href="https://medium.com/ixorthink/using-deep-q-learning-in-the-classification-of-an-imbalanced-dataset-22ee5e868efc" target="_blank" rel="noopener">Using Deep Q-Learning in the Classification of an Imbalanced Dataset</a></li>
<li><a href="https://www.zhihu.com/question/268939110/answer/351132564" target="_blank" rel="noopener">强化学习的Q-learning可以从已经完成的episode学习吗？还是边学习边完成episode的？ - 郭祥昊的回答 - 知乎</a></li>
<li><a href="https://www.novatec-gmbh.de/en/blog/introduction-to-q-learning/" target="_blank" rel="noopener">Reinforcement learning – Part 1: Introduction to Q-learning</a></li>
<li><a href="https://www.novatec-gmbh.de/en/blog/deep-q-networks/" target="_blank" rel="noopener">Reinforcement learning – Part 2: Getting started with Deep Q-Networks</a></li>
<li><a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html" target="_blank" rel="noopener">Deep Q Learning</a></li>
<li><a href="https://www.geeksforgeeks.org/deep-q-learning/" target="_blank" rel="noopener">Deep Q-Learning</a></li>
<li><a href="https://www.zhihu.com/question/26408259/answer/123230350" target="_blank" rel="noopener">如何用简单例子讲解 Q - learning 的具体过程？ - 牛阿的回答 - 知乎</a></li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">yinyoupoet</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yinyoupoet.github.io/2020/02/18/深度强化学习之深度Q网络DQN详解/">https://yinyoupoet.github.io/2020/02/18/深度强化学习之深度Q网络DQN详解/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yinyoupoet.github.io">yinyoupoet的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI    </a><a class="post-meta__tags" href="/tags/深度学习/">深度学习    </a><a class="post-meta__tags" href="/tags/强化学习/">强化学习    </a></div><div class="post_share"><div class="social-share" data-image="/images/cover&amp;top/DQN.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/images/cover&top/tenor.gif" data-original="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/images/cover&top/tenor.gif" data-original="/img/alipay.jpg"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2020/02/19/机器学习笔记/"><img class="prev_cover" src="/images/cover&top/tenor.gif" data-original="/images/cover&amp;top/DQN.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>机器学习笔记</span></div></a></div><div class="next-post pull-right"><a href="/2020/02/18/WireShark的使用/"><img class="next_cover" src="/images/cover&top/tenor.gif" data-original="/images/cover&amp;top/20190318170842_hYk5T.jpeg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>WireShark的使用</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/02/19/机器学习笔记/" title="机器学习笔记"><img class="relatedPosts_cover " src="/images/cover&top/tenor.gif" data-original="/images/cover&top/DQN.png"><div class="relatedPosts_title">机器学习笔记</div></a></div><div class="relatedPosts_item"><a href="/2019/11/05/Wireless-Network-Evolution-6G-AI/" title="Wireless Network Evolution - 6G & AI"><img class="relatedPosts_cover " src="/images/cover&top/tenor.gif" data-original="/images/cover&top/6G&AI.png"><div class="relatedPosts_title">Wireless Network Evolution - 6G & AI</div></a></div><div class="relatedPosts_item"><a href="/2019/10/06/5G与人工智能的应用场景分析与探索/" title="5G与人工智能的应用场景分析与探索"><img class="relatedPosts_cover " src="/images/cover&top/tenor.gif" data-original="/images/cover&top/20190318170842_hYk5T.jpeg"><div class="relatedPosts_title">5G与人工智能的应用场景分析与探索</div></a></div><div class="relatedPosts_item"><a href="/2019/10/05/从文献综述看AI与车联网的融合发展【附相关论文】/" title="从文献综述看AI与车联网的融合发展【附相关论文】"><img class="relatedPosts_cover " src="/images/cover&top/tenor.gif" data-original="/images/cover&top/shot-by-cerqueira-HEMgXMFpsAw-unsplash.jpg"><div class="relatedPosts_title">从文献综述看AI与车联网的融合发展【附相关论文】</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'smlw3k69ARv8sUTG6eFamB4N-gzGzoHsz',
  appKey:'yVLgeliGFjJizUR920FTQkj2',
  placeholder:'说点什么吧',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'zh-cn',
  recordIP: true
});</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By yinyoupoet</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">玻璃晴朗，橘子辉煌</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-book" id="readmode" title="阅读模式"> </i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="fa fa-moon-o nightshift" id="nightshift" title="夜间模式"></i></section><div id="post_bottom"><div id="post_bottom_items"><a id="mobile_to_comment" href="#post-comment"><i class="mobile_scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#强化学习"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> 强化学习</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#q-learning"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> Q-Learning</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#引例"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text"> 引例</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#q值更新方法"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text"> Q值更新方法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#s-a表格如何更新"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text"> S-A表格如何更新</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#策略policy选择注意事项"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text"> 策略Policy选择注意事项</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#dqn"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text"> DQN</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#dnn如何训练"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text"> DNN如何训练</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#experience-reply"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text"> Experience Reply</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#target-network"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text"> Target Network</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#参考资料"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text"> 参考资料</span></a></li></ol></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script async src="/js/search/algolia.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script><script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],0<=(n=t.getBoundingClientRect()).bottom&&0<=n.left&&n.top<=(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script></body></html>